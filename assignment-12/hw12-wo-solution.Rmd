---
title: "CSCI E-63C Week 12 Problem Set"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(ISLR)
library(e1071)
library(randomForest)
library(class)
library(ggplot2)
library(GGally)
knitr::opts_chunk$set(echo = TRUE)
```


# Preface

This week problem set will explore behavior of support vector classifiers and SVMs (following the distinction made in ISLR) on WiFi localization dataset from UCI ML archive.  We worked with it on multiple occasions before (most recently two weeks ago evaluating performance of logistic regression, discriminant analysis and KNN on it).  As two weeks ago we are going to convert the four-levels outcome in the data file to the binary one indicating localization at the third location:

```{r wifiExample,fig.width=8,fig.height=8,warning=FALSE}
wifiLocDat <- read.table("wifi_localization.txt",sep="\t")
colnames(wifiLocDat) <- c(paste0("WiFi",1:7),"Loc")
ggpairs(wifiLocDat,aes(colour=factor(Loc)))
wifiLocDat[,"Loc3"] <- factor(wifiLocDat[,"Loc"]==3)
wifiLocDat <- wifiLocDat[,colnames(wifiLocDat)!="Loc"]
dim(wifiLocDat)
summary(wifiLocDat)
head(wifiLocDat)
```

> Here we will use SVM implementation available in library `e1071` to fit classifiers with linear and radial (polynomial for extra points) kernels and compare their relative performance as well as to that of random forest and KNN.

# Problem 1 (20 points): support vector classifier (i.e. using linear kernel) 

> Use `svm` from library `e1071` with `kernel="linear"` to fit classifier (e.g. ISLR Ch.9.6.1) to the entire WiFi localization dataset setting parameter `cost` to 0.001, 1, 1000 and 1 mln.  


```{r svm1}
# cost=10, no scaling this time:

for (costVal in  c(0.001, 1, 1000, 1000000)) {
 
  print("------------------------------------------------------------")
  print(sprintf("Cost: %f", costVal));
  print("------------------------------------------------------------")
  svmfit2 <- svm(Loc3~., data=wifiLocDat, kernel="linear", cost=costVal, scale=FALSE)

  # Skip plotting, for now
  # plot(svmfit2, wifiLocDat)
  
  y = wifiLocDat[,"Loc3"]
  
  svmfit2$index
  print(summary(svmfit2))

  # actually three misclassifications
  print(table(predict(svmfit2), y))

  # plot(x,  col =(3-y), pch=as.numeric(predict(svmfit2)))
  # text(-0.9, 2.3-(0:2)/5, c("Truth:", as.character(unique(y))), col=c(1, unique(3-y)), pos=2)
  # text(2, -1.5, "Prediction:", pos=4)
  # legend("bottomright", levels(predict(svmfit2)), pch=1:2, bty="n")
}
```


> Describe how this change in parameter `cost` affects model fitting process (hint: the difficulty of the underlying optimization problem increases with cost -- can you explain what drives it?) and its outcome (how does the number of support vectors change with `cost`?) and what are the implications of that.

The algorithm is given another alternative (programmatically, this can create another if-else-branch) -- since it will now weigh the benefits of including or excluding data points near the dividing line.

> Explain why change in `cost` value impacts number of support vectors found. (Hint: there is an answer in ISLR.) 

The number of support vectors (misclassifications and points in the margin) decrease as cost increases.  _"An Introduction to Statistical Learning with Applications in R"_ (G. James, D. Witten, T. Hastie, R. Tibshirani)
page 347 in ISLR succinctly explains the reason for this: _When the tuning parameter C is large, then the margin is wide, many observations violate the margin, and so there are many support vectors_.

> Use `tune` function from library `e1071` (see ISLR Ch.9.6.1 for details and examples of usage) to determine approximate value of cost (in the range between 0.1 and 100 -- the suggested range spanning ordes of magnitude should hint that the density of the grid should be approximately logarithmic -- e.g. 1, 3, 10, ... or 1, 2, 5, 10, ... etc.) that yields the lowest error in cross-validation employed by `tune`.  Setup a resampling procedure repeatedly splitting entire dataset into training and test, using training data to `tune` cost value and test dataset to estimate classification error. Report and discuss distributions of test errors from this procedure and selected values of `cost`.

```{r tuneSVM}
# tune cost by cross-validation:
set.seed(1)
tune.out <- tune(svm, Loc3~., data=wifiLocDat, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
#tune.out <- tune(svm, Loc3~., data=wifiLocDat, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1)))
# cost=0.1 is the best:
summary(tune.out)
# best model:
bestmod <- tune.out$best.model
summary(bestmod)

# denser grid around minimum:
tune.out.1 <- tune(svm, Loc3~., data=wifiLocDat, kernel="linear", ranges=list(cost=c(0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1)))
summary(tune.out.1)
```

# Problem 2 (10 points): comparison to random forest

> Fit random forest classifier on the entire WiFi localization dataset with default parameters.  Calculate resulting misclassification error as reported by the confusion matrix in random forest output.  


```{r compareRandomForestWithSupportVectorMachine}

# Training ought to be larger than test set

nObs <- length(wifiLocDat[,"Loc3"])

# Get random indices
randomIndices <- sample(1:nObs, floor(nObs/2))

# Get indices not in randomIndices
randomIndicesInverse <- setdiff(seq(1:nObs), randomIndices)

xyzTrain <- wifiLocDat[randomIndices       , ]
xyzTest  <- wifiLocDat[randomIndicesInverse, ]

getRfTmpTable <- function(xyzTest, xyzTrain, classTest, classTrain) {
  rfRes <- randomForest(xyzTrain, classTrain)
  rfTmpTbl <- table(classTest, predict(rfRes, newdata=xyzTest))
  return (rfTmpTbl)
}

getSvmTmpTable <- function(xyzTest, xyzTrain, classTest, classTrain) {
  svmRes <- svm(xyzTrain, classTrain)
  svmTmpTbl <- table(classTest, predict(svmRes, newdata=xyzTest))
  return (svmTmpTbl)
}

rfResultTable <- getRfTmpTable(xyzTest[, 1:7], xyzTrain[, 1:7], xyzTest[,"Loc3"], xyzTrain[,"Loc3"])
rfResultTable
err = 1 - sum(diag(rfResultTable)) / sum(rfResultTable)
print(sprintf("Error rate for random forest: %f", err))

svmResultTable <- getSvmTmpTable(xyzTest[, 1:7], xyzTrain[, 1:7], xyzTest[,"Loc3"], xyzTrain[,"Loc3"])
svmResultTable
err = 1 - sum(diag(svmResultTable)) / sum(svmResultTable)
print(sprintf("Error rate for Scalable Vector Machines: %f", err))

```

> Explain why error reported in random forest confusion matrix represents estimated test (as opposed to train) error of the procedure.  

> Compare resulting test error to that for support vector classifier obtained above and discuss results of such comparison.

Neither SVM or Random Forest outperforms the other -- their error rates come within about 0.007 of each other.

# Problem 3 (10 points): Comparison to cross-validation tuned KNN predictor

> Use convenience wrapper `tune.knn` provided by the library `e1071` on the entire dataset to determine optimal value for the number of the nearest neighbors 'k' to be used in KNN classifier.  Consider our observations from week 9 problem set when choosing range of values of `k` to be evaluated by `tune.knn`.  Setup resampling procedure similar to that used above for support vector classifier that will repeatedly: 
>   a) split WiFi localization dataset into training and test, 
>   b) use `tune.knn` on training data to determine optimal `k`, and 
>   c) use `k` estimated by `tune.knn` to make KNN classifications on test data.  
>

```{r getKnnTmpTable}
# Fit LDA model to train data and evaluate error on the test data:

set.seed(1)
tune.out <- tune.knn(xyzTrain[, 1:7], xyzTrain[,"Loc3"], k = c(1, 2, 3, 4, 16, 32))

# cost=??? is the best:
summary(tune.out)
# best model:
bestmod <- tune.out$best.model
summary(bestmod)

# tune.knn recommends k = 16
knnRes <- knn(xyzTrain[, 1:7], xyzTest[, 1:7], xyzTrain[, 8], k=16)
knnTmpTbl <- table(xyzTest[, 8], knnRes)
knnTmpTbl
err = 1 - sum(diag(knnTmpTbl)) / sum(knnTmpTbl)
print(sprintf("Error rate for K nearest neighbor: %f", err))

```

> Report and discuss distributions of test errors from this procedure and selected values of `k`, compare them to those obtained for random forest and support vector classifier above.

With an error rate of 0.023, KNN (using the tuned parameter of k=16) performs roughly as well as random forest and SVM.

# Problem 4 (20 points): SVM with radial kernel

## Sub-problem 4a (10 points): impact of $gamma$ on classification surface

> *Plot* SVM model fit to the WiFi localization dataset using (for the ease of plotting) *only the first and the second attributes* as predictor variables, `kernel="radial"`, `cost=10` and `gamma=5` (see ISLR Ch.9.6.2 for an example of that done with a simulated dataset).  You should be able to see in the resulting plot the magenta-cyan (or, in more recent versions of `e1071` -- yellow-brown) classification boundary as computed by this model.  Produce the same kinds of plots using 0.5 and 50 as values of `gamma` also.  



```{r SvmWithRadialKernel}

for (gammaVal in c(0.5, 5, 50, 100, 200)) {
  
  # Use only the first and second attributes as predictor variables
  svmfit <- svm(Loc3~., data=wifiLocDat[,c(1,2,8)], kernel="radial", gamma = gammaVal, cost=10)
  print(sprintf("Gamma value: %f", gammaVal))
  plot(svmfit, wifiLocDat[,c(1,2,8)], sub=sprintf("gamma=%f", gammaVal))
  summary(svmfit)
  table(predict(svmfit), wifiLocDat[,8])
}

```

> Compare classification boundaries between these three plots and describe how they are impacted by the change in the value of `gamma`.

Increasing `gamma` tightens the magenta area.

> Can you trace it back to the role of `gamma` in the equation introducing it with the radial kernel in ISLR?

## Sub-problem 4b (10 points): test error for SVM with radial kernel

> Similar to how it was done above for support vector classifier (and KNN), set up a resampling process that will repeatedly: a) split the entire dataset (using all attributes as predictors) into training and test datasets, b) use `tune` function to determine optimal values of `cost` and `gamma` and c) calculate test error using these values of `cost` and `gamma`.    


```{r SvmWithRadialKernel2}

for ( iTry in 1:5 ) {
 
  nObs <- length(wifiLocDat[,"Loc3"])
 
  # Get random indices
  randomIndices <- sample(1:nObs, floor(nObs/2))
 
  # Get indices not in randomIndices
  randomIndicesInverse <- setdiff(seq(1:nObs), randomIndices)
 
  xyzTrain <- wifiLocDat[randomIndices, ]
  xyzTest  <- wifiLocDat[randomIndicesInverse, ]

  tune.out=tune(svm, Loc3~., data=xyzTrain, kernel="radial",ranges=list(cost=c(5,10,20,50,100,200), gamma=c(0,0.5,1,1.5,2)))
 
  print(tune.out$best.parameters)
  print(tune.out$best.performance)
  
  print(sprintf("tune() finds the following best parameters: cost = %f, gamma = %f",
          tune.out$best.parameters$cost,
          tune.out$best.parameters$gamma)
  )
  
  # Use the optimal parameters found in tune (see above) to run our test
  svmRes <- svm(xyzTrain[,1:7], xyzTrain[,8], cost=tune.out$best.parameters$cost, gamma=tune.out$best.parameters$gamma)
  svmTmpTbl <- table(xyzTest[,8], predict(svmRes, newdata=xyzTest[,1:7]))
  print(svmTmpTbl)  
  
  errRate <- 1-sum(diag(svmTmpTbl)) / sum(svmTmpTbl)
  print(sprintf("Error rate: %f", errRate))
}
```


> Consider what you have learned above about the effects of the parameters `cost` and `gamma` to decide on the starting ranges of their values to be evaluated by `tune`. 

I've passed the followign ranges of `cost` and `gamma` to `tune()`:
  cost  = c(5, 10, 20, 50, 100, 200)
  gamma = c(0, 0.5, 1, 1.5, 2)
 
`tune()` has selected 0.5 for gamma; and 5, 10, or 20 for cost -- achieving an error rate in the range 0.16 - 0.27.

> Additionally, experiment with different sets of their values and discuss in your solution the results of it and how you would go about selecting those ranges starting from scratch.


> Present resulting test error graphically, compare it to that of support vector classifier (with linear kernel), random forest and KNN classifiers obtained above and discuss results of these comparisons. 


# Extra 5 points problem: SVM with polynomial kernel

> Repeat what was done above (plots of decision boundaries for various interesting values of tuning parameters and test error for their best values estimated from training data) using `kernel="polynomial"`.   Determine ranges of `coef0`, `degree`, `cost` and `gamma` to be evaluated by `tune`.  Present and discuss resulting test error and how it compares to linear and radial kernels and those of random forest and KNN.

